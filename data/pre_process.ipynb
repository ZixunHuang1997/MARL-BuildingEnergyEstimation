{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc57ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf149d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '../data/img_residential_original/'\n",
    "all_data_dirs = os.listdir(data_root)\n",
    "all_data_dirs = [data_root + str(i) + '.png' for i in range(0,len(all_data_dirs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9330219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centeralize(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple): Desired output size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        tmp = image\n",
    "        image = torch.sum(image, dim=0)/3\n",
    "        h, w = image.shape\n",
    "        half_h = half_w = int(self.output_size/2)\n",
    "        grid_x = torch.FloatTensor([[i for i in range(0,w)] for j in range(0,h)])\n",
    "        grid_y = torch.FloatTensor([[j for i in range(0,w)] for j in range(0,h)])\n",
    "        img_reverse = (image<0.98).float()\n",
    "        center_w = int(torch.sum(img_reverse*grid_x)/torch.sum(img_reverse))\n",
    "        center_h = int(torch.sum(img_reverse*grid_y)/torch.sum(img_reverse))\n",
    "        print(center_h,center_w)\n",
    "\n",
    "        top = center_h - half_h\n",
    "        left = center_w - half_w\n",
    "        \n",
    "\n",
    "        image = tmp[:, top: top + self.output_size,\n",
    "                      left: left + self.output_size]\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bb68653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([\n",
    "                                Centeralize(1000),\n",
    "                                transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0)),\n",
    "                                transforms.Grayscale(1),\n",
    "                                transforms.Resize(112),\n",
    "                                transforms.CenterCrop(56),\n",
    "                              ])\n",
    "def saveto(i):\n",
    "    img = Image.open(all_data_dirs[i])\n",
    "    img = np.array(img)/255.0\n",
    "    img = np.transpose(img[:, :, :3], (2, 0, 1))\n",
    "    img_tensor = torch.from_numpy(img.astype(np.float32))\n",
    "    return composed(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d8905e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]] [[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "grid_x = [[i for i in range(0,5)] for j in range(0,3)]\n",
    "grid_y = [[j for i in range(0,5)] for j in range(0,3)]\n",
    "print(grid_x,grid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2e51ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637 1331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 0.4999, 0.5000,  ..., 0.4529, 0.5000, 0.5000],\n",
       "         [0.4999, 0.5000, 0.5000,  ..., 0.4529, 0.4999, 0.4999],\n",
       "         [0.5000, 0.4999, 0.4999,  ..., 0.4529, 0.1754, 0.4999],\n",
       "         ...,\n",
       "         [0.5000, 0.4999, 0.4529,  ..., 0.5000, 0.4999, 0.5000],\n",
       "         [0.5000, 0.4999, 0.4529,  ..., 0.4999, 0.4999, 0.5000],\n",
       "         [0.5000, 0.4999, 0.4529,  ..., 0.5000, 0.5000, 0.5000]]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveto(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aab48d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0739b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[1553,  353]],\n",
       " \n",
       "        [[1554,  352]],\n",
       " \n",
       "        [[1558,  352]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[1542,  354]],\n",
       " \n",
       "        [[1547,  354]],\n",
       " \n",
       "        [[1548,  353]]], dtype=int32),\n",
       " array([[[   0,    0]],\n",
       " \n",
       "        [[   0, 1256]],\n",
       " \n",
       "        [[2448, 1256]],\n",
       " \n",
       "        [[2448,    0]]], dtype=int32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e539b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4999, 0.4999, 0.4999,  ..., 0.4999, 0.4999, 0.4999],\n",
       "         [0.4999, 0.4999, 0.4999,  ..., 0.4999, 0.4999, 0.4999],\n",
       "         [0.4999, 0.4999, 0.5000,  ..., 0.5000, 0.5000, 0.4999],\n",
       "         ...,\n",
       "         [0.4999, 0.4999, 0.5000,  ..., 0.5000, 0.5000, 0.4999],\n",
       "         [0.4999, 0.4999, 0.5000,  ..., 0.5000, 0.5000, 0.4999],\n",
       "         [0.4999, 0.4999, 0.4999,  ..., 0.4999, 0.4999, 0.4999]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveto(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2cc44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(saveto(0), '../data/img_residential_original/0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c1954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch190",
   "language": "python",
   "name": "torch190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
